{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB1F9A8t1xHzRZSd629Zif",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aanantya/nlp/blob/main/generating_poetic_texts_using_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ddKXnBchOoAk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading 'shakespeare.txt', text corpus data to work with\n",
        "\n",
        "filepath = tf.keras.utils.get_file('shakespeare.txt',\n",
        "        'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "5PJUHSnXSWb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0159db10-c290-4203-8eee-592fd8bd7155"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read, convert the text content to lower-case\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()"
      ],
      "metadata": {
        "id": "WVKECQpMS4mj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model on a specific portion of text data, eg. from index:300000 -> index:800000\n",
        "text = text[300000:800000]"
      ],
      "metadata": {
        "id": "c7_zHG2rTKdA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# listing and sorting the text data as a sequence of characters\n",
        "characters = sorted(set(text))"
      ],
      "metadata": {
        "id": "kL2EXXhviLIB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the dictionary to map every 'char-to-index' and 'index-to-char'\n",
        "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
        "\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))"
      ],
      "metadata": {
        "id": "t9jXxOPYiVSv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the approach to learn from the data...\n",
        "# text will be split into sentence len of (start_index->to->40 char), the start_index = start_index + STEP_SIZE\n",
        "# next_character = start_index + SEQUENCE_LEN, i.e the next_character will point towards the next char of the sentence sequence\n",
        "# eg. sentences[0] = 'first citizen:\\nbefore we proceed any fur'\n",
        "#     next_characters[0] = 't'\n",
        "\n",
        "SEQUENCE_LEN = 40\n",
        "STEP_SIZE = 3\n",
        "\n",
        "sentences = []\n",
        "next_characters = []\n",
        "\n",
        "for i in range(0, len(text) - SEQUENCE_LEN, STEP_SIZE):\n",
        "  sentences.append(text[i: i+SEQUENCE_LEN])\n",
        "  next_characters.append(text[i+SEQUENCE_LEN])"
      ],
      "metadata": {
        "id": "4neqJLq8joOL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing a zeros matrix for fixed size sentence as input and expected next_character as output\n",
        "x = np.zeros((len(sentences), SEQUENCE_LEN, len(characters)))\n",
        "y = np.zeros((len(sentences), len(characters)))\n",
        "\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVXqhbxjqwRU",
        "outputId": "47748cb9-29e5-49fa-85be-aa9ad1636de2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(166654, 40, 39) (166654, 39)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the training data with 1/0\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    x[i, t, char_to_index[char]] = 1\n",
        "\n",
        "  y[i, char_to_index[next_characters[i]]] = 1"
      ],
      "metadata": {
        "id": "Z2cnJM78xy5x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "# LSTM for memory\n",
        "# Dense layer for the hidden layer\n",
        "# Activation layer for output layer\n",
        "# optimizer to compile all the layers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQUENCE_LEN, len(characters)))) # first layer\n",
        "model.add(Dense(len(characters))) # hidden layers with max neuron size = len(charcatres) -> 39\n",
        "model.add(Activation('softmax')) # output layer\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01))\n",
        "\n",
        "model.fit(x, y, batch_size=256, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEaK-4cn0VId",
        "outputId": "fae4064d-6a5c-425f-894f-d35c1138d4cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "651/651 [==============================] - 103s 155ms/step - loss: 2.1533\n",
            "Epoch 2/10\n",
            "651/651 [==============================] - 98s 151ms/step - loss: 1.7297\n",
            "Epoch 3/10\n",
            "651/651 [==============================] - 98s 151ms/step - loss: 1.5993\n",
            "Epoch 4/10\n",
            "651/651 [==============================] - 100s 154ms/step - loss: 1.5285\n",
            "Epoch 5/10\n",
            "651/651 [==============================] - 98s 151ms/step - loss: 1.4806\n",
            "Epoch 6/10\n",
            "651/651 [==============================] - 98s 150ms/step - loss: 1.4451\n",
            "Epoch 7/10\n",
            "651/651 [==============================] - 101s 155ms/step - loss: 1.4174\n",
            "Epoch 8/10\n",
            "651/651 [==============================] - 98s 151ms/step - loss: 1.3960\n",
            "Epoch 9/10\n",
            "651/651 [==============================] - 99s 152ms/step - loss: 1.3781\n",
            "Epoch 10/10\n",
            "651/651 [==============================] - 99s 153ms/step - loss: 1.3602\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bf0a52ae080>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHsDaZvTL3ZV",
        "outputId": "1076c4d5-9c0c-47ed-f244-71a778348c59"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 128)               86016     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 39)                5031      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 39)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 91047 (355.65 KB)\n",
            "Trainable params: 91047 (355.65 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model as 'textgenerator.model'\n",
        "model.save(\"textgenerator.model\")"
      ],
      "metadata": {
        "id": "Xps5A9rbHrPa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the saved 'textgenerator.model'\n",
        "model = tf.keras.models.load_model('textgenerator.model')\n",
        "\n",
        "\n",
        "# helper function offered by keras which help choice between the choices offered by softmax function\n",
        "# depending upon 'temperature' the value can be conservetive or experimental\n",
        "# temperature = 0 -> safe pick; temperature = 1 -> experimental choice(possible that selection won't make sense)\n",
        "def sample(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  prob = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(prob)\n",
        "\n",
        "\n",
        "def generate_text(length, temperature):\n",
        "  start_index = random.randint(0, len(text) - SEQUENCE_LEN - 1) # leaving the text length for atleast one sentence processing\n",
        "  generated_text = \"\" # empty\n",
        "  sentence = text[start_index: start_index + SEQUENCE_LEN]  # pick sentence starting from random index\n",
        "  generated_text += sentence  # base senetence to work on\n",
        "\n",
        "  for i in range(length):\n",
        "    x = np.zeros((1, SEQUENCE_LEN, len(characters)))\n",
        "    for t, char in enumerate(sentence):\n",
        "      x[0, t, char_to_index[char]] = 1  # sentence for prediction\n",
        "\n",
        "\n",
        "    prediction = model.predict(x, verbose=0)[0]\n",
        "\n",
        "    next_index = sample(prediction, temperature) # pick from prediction\n",
        "\n",
        "    next_character = index_to_char[next_index]  # convert index-to-char value\n",
        "\n",
        "    generated_text += next_character  # add char to base sentence\n",
        "\n",
        "    sentence = sentence[1:] + next_character # shift the sentence window\n",
        "\n",
        "  return generated_text"
      ],
      "metadata": {
        "id": "0V1B--MW8Cuu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ago1vrqMdI8d",
        "outputId": "912b6404-4acb-45bf-f539-6034582a311d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 128)               86016     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 39)                5031      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 39)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 91047 (355.65 KB)\n",
            "Trainable params: 91047 (355.65 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs at different temperature values\n",
        "print('-------------(300, 0.2)-------------')\n",
        "print(generate_text(300, 0.2))\n",
        "print('-------------(300, 0.4)-------------')\n",
        "print(generate_text(300, 0.4))\n",
        "print('-------------(300, 0.6)-------------')\n",
        "print(generate_text(300, 0.6))\n",
        "print('-------------(300, 0.8)-------------')\n",
        "print(generate_text(300, 0.8))\n",
        "print('-------------(300, 1.0)-------------')\n",
        "print(generate_text(300, 1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1eH43Tkdm1y",
        "outputId": "31ea4936-b739-4441-f1c5-1b7742161510"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------(300, 0.2)-------------\n",
            "over as they were gods or goddesses; you have the day the lands\n",
            "and the still the store of the world that they\n",
            "and the rest with the world that the stars\n",
            "the father hath a prince the looks of his life,\n",
            "and then the hands the tides the store of the still.\n",
            "\n",
            "polixenes:\n",
            "now the comes the starment to the beart\n",
            "the part the starst to see the gr\n",
            "-------------(300, 0.4)-------------\n",
            "ear, delivered with a groan,\n",
            "'o, farewell the still for the marrester'd the still.\n",
            "\n",
            "romeo:\n",
            "then will not still be setten a something lands\n",
            "and the lady speak the run suddest me a ster\n",
            "\n",
            "duke of york:\n",
            "sir, to see thy same the news with a summore.\n",
            "\n",
            "northumberland:\n",
            "what serve the hands me as the warth of the still\n",
            "the compliet, and to part th\n",
            "-------------(300, 0.6)-------------\n",
            "oncord of my state and time\n",
            "had not an earion and the 'ating sterity.\n",
            "\n",
            "king richard ii:\n",
            "be honouriet, and the bay thou wert with our mards,\n",
            "shall tears, my lord, the person as here,\n",
            "i have as my gracious frown of gracious.\n",
            "\n",
            "king richard ii:\n",
            "do master, and then, and set me was grant.\n",
            "\n",
            "lady capulet:\n",
            "hath then fight the sink me as he say.\n",
            "\n",
            "o\n",
            "-------------(300, 0.8)-------------\n",
            " your majesty\n",
            "than all the rest, discharge the kunestont.\n",
            "sliesife is he shall not see the poor from\n",
            "the base of his adield hard may with usour a durst,\n",
            "i'll be the middant rosthing and stand the son?\n",
            "\n",
            "paulina:\n",
            "see when, hear forch deadle will here stand.\n",
            "\n",
            "lady capulet:\n",
            "he's hear me, long your mercy are row now,\n",
            "when the day thy face to t\n",
            "-------------(300, 1.0)-------------\n",
            "and thrust his maids\n",
            "to the wall.\n",
            "\n",
            "gregoey:\n",
            "ip is my giars; for then yet to'd the day;\n",
            "my sprent the beoot-eye, stay them shall:\n",
            "who dost noinlick makes of might both, remorb.\n",
            "come, but where's upon my barreds slain;\n",
            "like edward; ay is marriagl samily,\n",
            "troath the hard's daying bud of mistress\n",
            "be pace abroad, that the duke of his duke,\n",
            "for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://www.neuralnine.com/generating-texts-with-recurrent-neural-networks-in-python/"
      ],
      "metadata": {
        "id": "aqGX3GvcTgTb"
      }
    }
  ]
}